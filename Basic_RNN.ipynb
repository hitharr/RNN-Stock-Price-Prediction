{"cells":[{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pyspark.mllib.evaluation import RegressionMetrics\n\nclass RecurrentNeuralNet:    \n  def __init__(self, inputFile, trainSplit, learningRate, numEpochs, hiddenDimensions, backPropTruncate, minClip, maxClip, timeSteps, activationFunction):\n    #load input file\n    inputData = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(inputFile)\n    \n    #save parameters for RNN for use in program\n    self.trainSplit = trainSplit\n    self.learningRate = learningRate\n    self.numEpochs = numEpochs\n    self.hiddenDimensions = hiddenDimensions\n    self.outputDimension = 1\n    self.backPropTruncate = backPropTruncate\n    self.minClip = minClip\n    self.maxClip = maxClip\n    self.timeSteps = timeSteps\n    self.activationFunction  = activationFunction\n    \n    #get size of train and test data based on split\n    trainSize = int(inputData.count() * self.trainSplit)\n    testSize = inputData.count()- trainSize\n    \n    #preprocessing to scale data\n    inputData = inputData.toPandas()\n    self.closeScaler =MinMaxScaler(feature_range=(0,1))\n    inputData[['Close']] = self.closeScaler.fit_transform(inputData.loc[:, ['Close']])\n    closingStockPrice = inputData['Close'].tolist()\n\n    #split into test and train set   \n    self.train = closingStockPrice[:trainSize]\n    self.test = closingStockPrice[-testSize:]\n    \n    #generate sequences of data to train on for train and test\n    self.inputTrain, self.outputTrain = self.generateSeqs(self.train, self.timeSteps)\n    self.inputTest, self.outputTest = self.generateSeqs(self.test, self.timeSteps)\n    \n    #generate random starting between 0 and 1 \n    self.U= np.random.rand(hiddenDimensions, timeSteps)\n    self.W = np.random.rand(hiddenDimensions, hiddenDimensions)\n    self.V = np.random.rand(self.outputDimension, hiddenDimensions)\n\n  #generate sequences of input and output data to perform training and testing on \n  def generateSeqs(self, input, steps):\n    inputData = []\n    targetData = []\n    for x in range (len(input) - steps):\n      #get series of data\n      inputData.append(input[x: x + steps])\n      #get target value to predict\n      targetData.append(input[x + steps])\n    return np.expand_dims(np.array(inputData), axis = 2), np.expand_dims(np.array(targetData), axis=1)\n  \n  \n  #define activation functions and derivatives\n  def sigmoid(self,x):\n    return 1/(1+np.exp(-x))\n\n  def sigmoidDerivative(self,x):\n    return x*(1-x)\n\n  def tanh(self, x):\n    return np.tanh(x)\n\n  def tanhDerivative(self, x):\n    return 1-x**2\n  \n  def activation(self, x):\n    if self.activationFunction == \"sigmoid\":\n      return self.sigmoid(x)\n    elif self.activationFunction == \"tanh\":\n      return self.tanh(x)\n  \n  def activationDerivative(self, x):\n    if self.activationFunction == \"sigmoid\":\n      return self.sigmoidDerivative(x)\n    elif self.activationFunction == \"tanh\":\n      return self.tanhDerivative(x)\n  \n\n  #simple forward pass\n  def forward(self, numSequences, X, Y):\n    loss = 0\n    for i in range(numSequences):\n      x, y = X[i], Y[i]                    \n      prevHidden = np.zeros((self.hiddenDimensions, 1))  \n      for t in range(self.timeSteps):\n          input = np.zeros(x.shape)    \n          input[t] = x[t]             \n          hiddenState = self.activation(np.dot(self.U, input) + np.dot(self.W, prevHidden))\n          outputState = np.dot(self.V, hiddenState)\n          prevHidden = hiddenState\n      loss += (y - outputState)**2\n    loss = loss/float(self.outputTrain.shape[0])  \n    return x, y, outputState, loss\n   \n  #forward pass keeping track of layers between timesteps\n  def forwardWithLayers(self, x):\n    prevHidden = np.zeros((hiddenDimensions, 1))\n    self.layers = []\n    for t in range(self.timeSteps):\n      input = np.zeros(x.shape)\n      input[t] = x[t]\n      sum = np.dot(self.U, input) + np.dot(self.W, prevHidden)\n      hiddenState = self.activation(sum)\n      outputState = np.dot(self.V, hiddenState)\n      self.layers.append({'hiddenState':hiddenState, 'prevHidden':prevHidden})\n      prevHidden = hiddenState\n    return x, sum, outputState, prevHidden\n  \n  def trainModel(self):\n    for i in range(self.outputTrain.shape[0]):\n      x, y = self.inputTrain[i], self.outputTrain[i]\n      #create variables to hold gradient values\n      dU = np.zeros(self.U.shape)\n      dV = np.zeros(self.V.shape)\n      dW = np.zeros(self.W.shape)\n      \n      #create vars to help calculate gradient\n      dUtime = np.zeros(self.U.shape)\n      dVtime = np.zeros(self.V.shape)\n      dWtime = np.zeros(self.W.shape)\n      \n      #forward pass\n      x, sum, outputState, prevHidden = self.forwardWithLayers(x)\n    dOut = (outputState-y)\n  \n    #backward pass\n    for t in range(self.timeSteps):\n      dVtime = np.dot(dOut, np.transpose(self.layers[t]['hiddenState']))\n      dOutStep = np.dot(np.transpose(self.V), dOut)\n      dSumAct = self.activationDerivative(sum) * dOutStep\n      dPrevHidden = np.dot(np.transpose(self.W), dSumAct * np.ones_like((self.hiddenDimensions, 1)))\n      #backprop through time\n      for i in reversed(range(t-1, t-backPropTruncate-1)):\n        dSumAct = self.activationDerivative(sum) * (dOutStep + dPrevHidden)\n        dWiterTime = np.dot(self.W, self.layers[t]['prevHidden'])\n        dPrevHidden = np.dot(np.transpose(self.W), dSumAct * np.ones_like((self.hiddenDimensions, 1)))\n        input = np.zeros(x.shape)\n        input[t] = x[t]\n\n        dUtime += np.dot(self.U, input)\n        dWtime += np.dot(self.W, self.layers[t]['prevHidden'])\n        \n      #update gradient weights  \n      dV += dVtime\n      dU += dUtime\n      dW += dWtime\n      \n      #perform gradient clipping to prevent exploding gradient\n      dU = dU.clip(minClip, maxClip)\n      dV = dV.clip(minClip, maxClip)\n      dW = dW.clip(minClip, maxClip)\n      \n    #update weights\n    self.U -= self.learningRate * dU\n    self.V -= self.learningRate * dV\n    self.W -= self.learningRate * dW   \n\n  def fitModel(self):\n    for e in range(self.numEpochs):\n      # check loss on training data\n      x, y, outputState, trainLoss = self.forward(self.outputTrain.shape[0], self.inputTrain, self.outputTrain)\n      # check loss based on test set\n      x, y, outputState, testLoss= self.forward(self.outputTest.shape[0], self.inputTest, self.outputTest )\n      print('Epoch: ', e + 1, ', Training Loss: ', trainLoss, ', Test Loss: ', testLoss)    \n      self.trainModel()\n      \n  def makePredictions(self, numSequences, input):\n    preds = []\n    for i in range(numSequences):\n        x = input[i]\n        prevHidden = np.zeros((self.hiddenDimensions, 1))\n        for t in range(self.timeSteps):\n            hiddenState = self.activation(np.dot(self.U, x) + np.dot(self.W, prevHidden))\n            outputState = np.dot(self.V, hiddenState)\n            prevHidden = hiddenState\n        preds.append(outputState)\n    preds = np.array(preds)\n    reshapePreds = pd.DataFrame.from_records(preds.reshape(-1,1))\n    return self.closeScaler.inverse_transform(reshapePreds)\n\n  def evaluate(self):   \n    trainPredictions = self.makePredictions(self.outputTrain.shape[0], self.inputTrain)\n    trainActual = self.closeScaler.inverse_transform(self.outputTrain)\n    trainMetrics = self.calculateRMSE(trainPredictions, trainActual)\n    print(\"Training RMSE = %s\" % trainMetrics)\n    \n    testPredictions = self.makePredictions(self.outputTest.shape[0], self.inputTest)\n    testActual = self.closeScaler.inverse_transform(self.outputTest)\n    testMetrics = self.calculateRMSE(testPredictions, testActual)\n    print(\"Test RMSE = %s\" % testMetrics)\n\n  \n  def calculateRMSE(self, predictions, actual):\n    predictionsList  = predictions.ravel().tolist()\n    actualList = actual.ravel().tolist()\n    mergeLists = list(zip(predictionsList, actualList))\n    predictionsAndActual = sc.parallelize(mergeLists)\n    predictionsAndActual.collect()\n    metrics = RegressionMetrics(predictionsAndActual)\n    return metrics.rootMeanSquaredError\n    \n\n\n "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c795e69d-c23d-4731-bb00-b9c822982307"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[" %sh \n curl -O https://s3.amazonaws.com/cs6350.0u1/Project/infy_stock.csv"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c63c7bc-c0ab-449a-80bb-86d6519f1555"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 30177  100 30177    0     0  58378      0 --:--:-- --:--:-- --:--:-- 58482\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 30177  100 30177    0     0  58378      0 --:--:-- --:--:-- --:--:-- 58482\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["inputFile = \"file:/databricks/driver/infy_stock.csv\"\n\n#Params that can be changed\ntrainSplit = 0.8\nearningRate = 0.001\nnumEpochs =  100\nhiddenDimensions = 100\nbackPropTruncate = 5\nminClip = -10\nmaxClip = 10\ntimeSteps = 15;\nactivationFunction = \"tanh\"\n\nrnn = RecurrentNeuralNet(inputFile, trainSplit, learningRate, numEpochs, hiddenDimensions, backPropTruncate, minClip, maxClip, timeSteps, activationFunction)\nrnn.fitModel()\nrnn.evaluate()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a446c0f-6e6b-419e-a853-8fa3e7b5db2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Epoch:  1 , Training Loss:  [[2550.91185813]] , Test Loss:  [[495.63981602]]\nEpoch:  2 , Training Loss:  [[2450.90176236]] , Test Loss:  [[476.35858063]]\nEpoch:  3 , Training Loss:  [[2352.8916666]] , Test Loss:  [[457.4598589]]\nEpoch:  4 , Training Loss:  [[2256.88157083]] , Test Loss:  [[438.94365083]]\nEpoch:  5 , Training Loss:  [[2162.87147507]] , Test Loss:  [[420.80995642]]\nEpoch:  6 , Training Loss:  [[2070.8613793]] , Test Loss:  [[403.05877568]]\nEpoch:  7 , Training Loss:  [[1980.85128353]] , Test Loss:  [[385.69010859]]\nEpoch:  8 , Training Loss:  [[1892.84118777]] , Test Loss:  [[368.70395516]]\nEpoch:  9 , Training Loss:  [[1806.831092]] , Test Loss:  [[352.1003154]]\nEpoch:  10 , Training Loss:  [[1722.82099624]] , Test Loss:  [[335.8791893]]\nEpoch:  11 , Training Loss:  [[1640.81090047]] , Test Loss:  [[320.04057686]]\nEpoch:  12 , Training Loss:  [[1560.80080471]] , Test Loss:  [[304.58447808]]\nEpoch:  13 , Training Loss:  [[1482.79070894]] , Test Loss:  [[289.51089296]]\nEpoch:  14 , Training Loss:  [[1406.78061318]] , Test Loss:  [[274.8198215]]\nEpoch:  15 , Training Loss:  [[1332.77051741]] , Test Loss:  [[260.5112637]]\nEpoch:  16 , Training Loss:  [[1260.76042164]] , Test Loss:  [[246.58521957]]\nEpoch:  17 , Training Loss:  [[1190.75032588]] , Test Loss:  [[233.04168909]]\nEpoch:  18 , Training Loss:  [[1122.74023011]] , Test Loss:  [[219.88067228]]\nEpoch:  19 , Training Loss:  [[1056.73013435]] , Test Loss:  [[207.10216913]]\nEpoch:  20 , Training Loss:  [[992.72003858]] , Test Loss:  [[194.70617964]]\nEpoch:  21 , Training Loss:  [[930.70994282]] , Test Loss:  [[182.69270381]]\nEpoch:  22 , Training Loss:  [[870.69984705]] , Test Loss:  [[171.06174164]]\nEpoch:  23 , Training Loss:  [[812.68975128]] , Test Loss:  [[159.81329313]]\nEpoch:  24 , Training Loss:  [[756.67965552]] , Test Loss:  [[148.94735828]]\nEpoch:  25 , Training Loss:  [[702.66955975]] , Test Loss:  [[138.4639371]]\nEpoch:  26 , Training Loss:  [[650.65946399]] , Test Loss:  [[128.36302958]]\nEpoch:  27 , Training Loss:  [[600.64936822]] , Test Loss:  [[118.64463571]]\nEpoch:  28 , Training Loss:  [[552.63927246]] , Test Loss:  [[109.30875551]]\nEpoch:  29 , Training Loss:  [[506.62917669]] , Test Loss:  [[100.35538897]]\nEpoch:  30 , Training Loss:  [[462.61908093]] , Test Loss:  [[91.78453609]]\nEpoch:  31 , Training Loss:  [[420.60898516]] , Test Loss:  [[83.59619688]]\nEpoch:  32 , Training Loss:  [[380.59888939]] , Test Loss:  [[75.79037132]]\nEpoch:  33 , Training Loss:  [[342.58879363]] , Test Loss:  [[68.36705943]]\nEpoch:  34 , Training Loss:  [[306.57869786]] , Test Loss:  [[61.32626119]]\nEpoch:  35 , Training Loss:  [[272.5686021]] , Test Loss:  [[54.66797662]]\nEpoch:  36 , Training Loss:  [[240.55850633]] , Test Loss:  [[48.39220571]]\nEpoch:  37 , Training Loss:  [[210.54841057]] , Test Loss:  [[42.49894846]]\nEpoch:  38 , Training Loss:  [[182.5383148]] , Test Loss:  [[36.98820487]]\nEpoch:  39 , Training Loss:  [[156.52821904]] , Test Loss:  [[31.85997494]]\nEpoch:  40 , Training Loss:  [[132.51812327]] , Test Loss:  [[27.11425867]]\nEpoch:  41 , Training Loss:  [[110.5080275]] , Test Loss:  [[22.75105607]]\nEpoch:  42 , Training Loss:  [[90.49793174]] , Test Loss:  [[18.77036712]]\nEpoch:  43 , Training Loss:  [[72.48783597]] , Test Loss:  [[15.17219184]]\nEpoch:  44 , Training Loss:  [[56.47774021]] , Test Loss:  [[11.95653022]]\nEpoch:  45 , Training Loss:  [[42.46764444]] , Test Loss:  [[9.12338226]]\nEpoch:  46 , Training Loss:  [[30.45754868]] , Test Loss:  [[6.67274796]]\nEpoch:  47 , Training Loss:  [[20.44745291]] , Test Loss:  [[4.60462732]]\nEpoch:  48 , Training Loss:  [[12.43735715]] , Test Loss:  [[2.91902034]]\nEpoch:  49 , Training Loss:  [[6.42726138]] , Test Loss:  [[1.61592703]]\nEpoch:  50 , Training Loss:  [[2.41716561]] , Test Loss:  [[0.69534737]]\nEpoch:  51 , Training Loss:  [[0.40706985]] , Test Loss:  [[0.15728138]]\nEpoch:  52 , Training Loss:  [[0.39697408]] , Test Loss:  [[0.00172905]]\nEpoch:  53 , Training Loss:  [[0.26872259]] , Test Loss:  [[0.00075026]]\nEpoch:  54 , Training Loss:  [[0.31513106]] , Test Loss:  [[6.36885164e-05]]\nEpoch:  55 , Training Loss:  [[0.29535252]] , Test Loss:  [[0.00016374]]\nEpoch:  56 , Training Loss:  [[0.30322948]] , Test Loss:  [[9.36485475e-05]]\nEpoch:  57 , Training Loss:  [[0.30000565]] , Test Loss:  [[0.00011725]]\nEpoch:  58 , Training Loss:  [[0.3013105]] , Test Loss:  [[0.00010687]]\nEpoch:  59 , Training Loss:  [[0.30077998]] , Test Loss:  [[0.00011095]]\nEpoch:  60 , Training Loss:  [[0.30099528]] , Test Loss:  [[0.00010927]]\nEpoch:  61 , Training Loss:  [[0.30090784]] , Test Loss:  [[0.00010995]]\nEpoch:  62 , Training Loss:  [[0.30094334]] , Test Loss:  [[0.00010967]]\nEpoch:  63 , Training Loss:  [[0.30092892]] , Test Loss:  [[0.00010979]]\nEpoch:  64 , Training Loss:  [[0.30093478]] , Test Loss:  [[0.00010974]]\nEpoch:  65 , Training Loss:  [[0.3009324]] , Test Loss:  [[0.00010976]]\nEpoch:  66 , Training Loss:  [[0.30093337]] , Test Loss:  [[0.00010975]]\nEpoch:  67 , Training Loss:  [[0.30093297]] , Test Loss:  [[0.00010975]]\nEpoch:  68 , Training Loss:  [[0.30093313]] , Test Loss:  [[0.00010975]]\nEpoch:  69 , Training Loss:  [[0.30093307]] , Test Loss:  [[0.00010975]]\nEpoch:  70 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  71 , Training Loss:  [[0.30093308]] , Test Loss:  [[0.00010975]]\nEpoch:  72 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  73 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  74 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  75 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  76 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  77 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  78 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  79 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  80 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  81 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  82 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  83 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  84 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  85 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  86 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  87 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  88 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  89 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  90 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  91 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  92 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  93 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  94 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  95 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  96 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  97 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  98 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  99 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  100 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nTraining RMSE = 760.9814149012683\nTest RMSE = 33.230681820957685\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch:  1 , Training Loss:  [[2550.91185813]] , Test Loss:  [[495.63981602]]\nEpoch:  2 , Training Loss:  [[2450.90176236]] , Test Loss:  [[476.35858063]]\nEpoch:  3 , Training Loss:  [[2352.8916666]] , Test Loss:  [[457.4598589]]\nEpoch:  4 , Training Loss:  [[2256.88157083]] , Test Loss:  [[438.94365083]]\nEpoch:  5 , Training Loss:  [[2162.87147507]] , Test Loss:  [[420.80995642]]\nEpoch:  6 , Training Loss:  [[2070.8613793]] , Test Loss:  [[403.05877568]]\nEpoch:  7 , Training Loss:  [[1980.85128353]] , Test Loss:  [[385.69010859]]\nEpoch:  8 , Training Loss:  [[1892.84118777]] , Test Loss:  [[368.70395516]]\nEpoch:  9 , Training Loss:  [[1806.831092]] , Test Loss:  [[352.1003154]]\nEpoch:  10 , Training Loss:  [[1722.82099624]] , Test Loss:  [[335.8791893]]\nEpoch:  11 , Training Loss:  [[1640.81090047]] , Test Loss:  [[320.04057686]]\nEpoch:  12 , Training Loss:  [[1560.80080471]] , Test Loss:  [[304.58447808]]\nEpoch:  13 , Training Loss:  [[1482.79070894]] , Test Loss:  [[289.51089296]]\nEpoch:  14 , Training Loss:  [[1406.78061318]] , Test Loss:  [[274.8198215]]\nEpoch:  15 , Training Loss:  [[1332.77051741]] , Test Loss:  [[260.5112637]]\nEpoch:  16 , Training Loss:  [[1260.76042164]] , Test Loss:  [[246.58521957]]\nEpoch:  17 , Training Loss:  [[1190.75032588]] , Test Loss:  [[233.04168909]]\nEpoch:  18 , Training Loss:  [[1122.74023011]] , Test Loss:  [[219.88067228]]\nEpoch:  19 , Training Loss:  [[1056.73013435]] , Test Loss:  [[207.10216913]]\nEpoch:  20 , Training Loss:  [[992.72003858]] , Test Loss:  [[194.70617964]]\nEpoch:  21 , Training Loss:  [[930.70994282]] , Test Loss:  [[182.69270381]]\nEpoch:  22 , Training Loss:  [[870.69984705]] , Test Loss:  [[171.06174164]]\nEpoch:  23 , Training Loss:  [[812.68975128]] , Test Loss:  [[159.81329313]]\nEpoch:  24 , Training Loss:  [[756.67965552]] , Test Loss:  [[148.94735828]]\nEpoch:  25 , Training Loss:  [[702.66955975]] , Test Loss:  [[138.4639371]]\nEpoch:  26 , Training Loss:  [[650.65946399]] , Test Loss:  [[128.36302958]]\nEpoch:  27 , Training Loss:  [[600.64936822]] , Test Loss:  [[118.64463571]]\nEpoch:  28 , Training Loss:  [[552.63927246]] , Test Loss:  [[109.30875551]]\nEpoch:  29 , Training Loss:  [[506.62917669]] , Test Loss:  [[100.35538897]]\nEpoch:  30 , Training Loss:  [[462.61908093]] , Test Loss:  [[91.78453609]]\nEpoch:  31 , Training Loss:  [[420.60898516]] , Test Loss:  [[83.59619688]]\nEpoch:  32 , Training Loss:  [[380.59888939]] , Test Loss:  [[75.79037132]]\nEpoch:  33 , Training Loss:  [[342.58879363]] , Test Loss:  [[68.36705943]]\nEpoch:  34 , Training Loss:  [[306.57869786]] , Test Loss:  [[61.32626119]]\nEpoch:  35 , Training Loss:  [[272.5686021]] , Test Loss:  [[54.66797662]]\nEpoch:  36 , Training Loss:  [[240.55850633]] , Test Loss:  [[48.39220571]]\nEpoch:  37 , Training Loss:  [[210.54841057]] , Test Loss:  [[42.49894846]]\nEpoch:  38 , Training Loss:  [[182.5383148]] , Test Loss:  [[36.98820487]]\nEpoch:  39 , Training Loss:  [[156.52821904]] , Test Loss:  [[31.85997494]]\nEpoch:  40 , Training Loss:  [[132.51812327]] , Test Loss:  [[27.11425867]]\nEpoch:  41 , Training Loss:  [[110.5080275]] , Test Loss:  [[22.75105607]]\nEpoch:  42 , Training Loss:  [[90.49793174]] , Test Loss:  [[18.77036712]]\nEpoch:  43 , Training Loss:  [[72.48783597]] , Test Loss:  [[15.17219184]]\nEpoch:  44 , Training Loss:  [[56.47774021]] , Test Loss:  [[11.95653022]]\nEpoch:  45 , Training Loss:  [[42.46764444]] , Test Loss:  [[9.12338226]]\nEpoch:  46 , Training Loss:  [[30.45754868]] , Test Loss:  [[6.67274796]]\nEpoch:  47 , Training Loss:  [[20.44745291]] , Test Loss:  [[4.60462732]]\nEpoch:  48 , Training Loss:  [[12.43735715]] , Test Loss:  [[2.91902034]]\nEpoch:  49 , Training Loss:  [[6.42726138]] , Test Loss:  [[1.61592703]]\nEpoch:  50 , Training Loss:  [[2.41716561]] , Test Loss:  [[0.69534737]]\nEpoch:  51 , Training Loss:  [[0.40706985]] , Test Loss:  [[0.15728138]]\nEpoch:  52 , Training Loss:  [[0.39697408]] , Test Loss:  [[0.00172905]]\nEpoch:  53 , Training Loss:  [[0.26872259]] , Test Loss:  [[0.00075026]]\nEpoch:  54 , Training Loss:  [[0.31513106]] , Test Loss:  [[6.36885164e-05]]\nEpoch:  55 , Training Loss:  [[0.29535252]] , Test Loss:  [[0.00016374]]\nEpoch:  56 , Training Loss:  [[0.30322948]] , Test Loss:  [[9.36485475e-05]]\nEpoch:  57 , Training Loss:  [[0.30000565]] , Test Loss:  [[0.00011725]]\nEpoch:  58 , Training Loss:  [[0.3013105]] , Test Loss:  [[0.00010687]]\nEpoch:  59 , Training Loss:  [[0.30077998]] , Test Loss:  [[0.00011095]]\nEpoch:  60 , Training Loss:  [[0.30099528]] , Test Loss:  [[0.00010927]]\nEpoch:  61 , Training Loss:  [[0.30090784]] , Test Loss:  [[0.00010995]]\nEpoch:  62 , Training Loss:  [[0.30094334]] , Test Loss:  [[0.00010967]]\nEpoch:  63 , Training Loss:  [[0.30092892]] , Test Loss:  [[0.00010979]]\nEpoch:  64 , Training Loss:  [[0.30093478]] , Test Loss:  [[0.00010974]]\nEpoch:  65 , Training Loss:  [[0.3009324]] , Test Loss:  [[0.00010976]]\nEpoch:  66 , Training Loss:  [[0.30093337]] , Test Loss:  [[0.00010975]]\nEpoch:  67 , Training Loss:  [[0.30093297]] , Test Loss:  [[0.00010975]]\nEpoch:  68 , Training Loss:  [[0.30093313]] , Test Loss:  [[0.00010975]]\nEpoch:  69 , Training Loss:  [[0.30093307]] , Test Loss:  [[0.00010975]]\nEpoch:  70 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  71 , Training Loss:  [[0.30093308]] , Test Loss:  [[0.00010975]]\nEpoch:  72 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  73 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  74 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  75 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  76 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  77 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  78 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  79 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  80 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  81 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  82 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  83 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  84 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  85 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  86 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  87 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  88 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  89 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  90 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  91 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  92 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  93 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  94 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  95 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  96 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  97 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  98 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  99 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nEpoch:  100 , Training Loss:  [[0.30093309]] , Test Loss:  [[0.00010975]]\nTraining RMSE = 760.9814149012683\nTest RMSE = 33.230681820957685\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"RNN Project Final Cleaned Up","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":660925766944664}},"nbformat":4,"nbformat_minor":0}
